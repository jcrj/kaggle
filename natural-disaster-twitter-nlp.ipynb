{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-07-14T14:20:39.954103Z","iopub.execute_input":"2023-07-14T14:20:39.954489Z","iopub.status.idle":"2023-07-14T14:20:39.969668Z","shell.execute_reply.started":"2023-07-14T14:20:39.954462Z","shell.execute_reply":"2023-07-14T14:20:39.968601Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport string\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nsns.set()\nsns.set_style('darkgrid')\nsns.color_palette('husl')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:39.973154Z","iopub.execute_input":"2023-07-14T14:20:39.974197Z","iopub.status.idle":"2023-07-14T14:20:42.323775Z","shell.execute_reply.started":"2023-07-14T14:20:39.974158Z","shell.execute_reply":"2023-07-14T14:20:42.322641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:42.330278Z","iopub.execute_input":"2023-07-14T14:20:42.333276Z","iopub.status.idle":"2023-07-14T14:20:42.455034Z","shell.execute_reply.started":"2023-07-14T14:20:42.333231Z","shell.execute_reply":"2023-07-14T14:20:42.454104Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"id - a unique identifier for each tweet  \ntext - the text of the tweet  \nlocation - the location the tweet was sent from (may be blank)  \nkeyword - a particular keyword from the tweet (may be blank)  \ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)  ","metadata":{}},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:42.459849Z","iopub.execute_input":"2023-07-14T14:20:42.462193Z","iopub.status.idle":"2023-07-14T14:20:42.488042Z","shell.execute_reply.started":"2023-07-14T14:20:42.462157Z","shell.execute_reply":"2023-07-14T14:20:42.486792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:42.493157Z","iopub.execute_input":"2023-07-14T14:20:42.496060Z","iopub.status.idle":"2023-07-14T14:20:42.520715Z","shell.execute_reply.started":"2023-07-14T14:20:42.496023Z","shell.execute_reply":"2023-07-14T14:20:42.519792Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"missing_cols = ['keyword', 'location']\nfig, axes = plt.subplots(1,2, figsize = (10,6))\nsns.barplot(x = train[missing_cols].isnull().sum().index, y = train[missing_cols].isnull().sum().values, ax = axes[0])\nsns.barplot(x = test[missing_cols].isnull().sum().index, y = test[missing_cols].isnull().sum().values, ax = axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:42.525234Z","iopub.execute_input":"2023-07-14T14:20:42.528244Z","iopub.status.idle":"2023-07-14T14:20:43.159739Z","shell.execute_reply.started":"2023-07-14T14:20:42.528207Z","shell.execute_reply":"2023-07-14T14:20:43.158775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def fill_na(df):\n    \"\"\"\n    Fill the null values in the datasets.\n    \"\"\"\n    df['keyword'] = df['keyword'].fillna('None')\n    df['location'] = df['location'].fillna('None')\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:43.164264Z","iopub.execute_input":"2023-07-14T14:20:43.166647Z","iopub.status.idle":"2023-07-14T14:20:43.174056Z","shell.execute_reply.started":"2023-07-14T14:20:43.166611Z","shell.execute_reply":"2023-07-14T14:20:43.173005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_train = fill_na(train)\np1_train.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:43.178508Z","iopub.execute_input":"2023-07-14T14:20:43.181024Z","iopub.status.idle":"2023-07-14T14:20:43.207739Z","shell.execute_reply.started":"2023-07-14T14:20:43.180988Z","shell.execute_reply":"2023-07-14T14:20:43.206916Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"For now, I won't deal with location and keywords.","metadata":{}},{"cell_type":"code","source":"p1_test = fill_na(test)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:43.210784Z","iopub.execute_input":"2023-07-14T14:20:43.211636Z","iopub.status.idle":"2023-07-14T14:20:43.220260Z","shell.execute_reply.started":"2023-07-14T14:20:43.211598Z","shell.execute_reply":"2023-07-14T14:20:43.218840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train, y = p1_train.drop(['id', 'location', 'keyword', 'target'], axis = 1).iloc[:, 0], p1_train['target']\nX_test = p1_test.drop(['id', 'location', 'keyword'], axis = 1)\nX_train.shape, y.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:43.223087Z","iopub.execute_input":"2023-07-14T14:20:43.223895Z","iopub.status.idle":"2023-07-14T14:20:43.234573Z","shell.execute_reply.started":"2023-07-14T14:20:43.223842Z","shell.execute_reply":"2023-07-14T14:20:43.233643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\ndef calculate_results(y_true, y_pred):\n\n    # calculate metrics \n    acc = accuracy_score(y_true, y_pred)\n    f1 = f1_score(y_true, y_pred)\n    precision = precision_score(y_true, y_pred)\n    recall = recall_score(y_true, y_pred)\n    return {'accuracy': acc, 'f1': f1, 'precision': precision, 'recall': recall}","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:43.239821Z","iopub.execute_input":"2023-07-14T14:20:43.240502Z","iopub.status.idle":"2023-07-14T14:20:43.709632Z","shell.execute_reply.started":"2023-07-14T14:20:43.240467Z","shell.execute_reply":"2023-07-14T14:20:43.708458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nX_train_1, X_val_1, y_train_1, y_val_1 = train_test_split(X_train, y, test_size = 0.2, random_state = 42)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:43.715182Z","iopub.execute_input":"2023-07-14T14:20:43.718560Z","iopub.status.idle":"2023-07-14T14:20:43.750410Z","shell.execute_reply.started":"2023-07-14T14:20:43.718501Z","shell.execute_reply":"2023-07-14T14:20:43.749423Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Initialize vectorizers and pipelines\ncount_vec = CountVectorizer()\ntfidf_vec = TfidfVectorizer()\n\npipe_countvec = make_pipeline(count_vec, MultinomialNB())\npipe_tfidfvec = make_pipeline(tfidf_vec, MultinomialNB())\n\n# Fit the models\npipe_countvec.fit(X_train_1, y_train_1)\npipe_tfidfvec.fit(X_train_1, y_train_1)\n\n# Initialize a dictionary to store the results\nmodels = {}\n\n# Predict and calculate results\nmodels['countvec_baseline'] = calculate_results(y_val_1, pipe_countvec.predict(X_val_1))\nmodels['tfidf_baseline'] = calculate_results(y_val_1, pipe_tfidfvec.predict(X_val_1))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:43.754733Z","iopub.execute_input":"2023-07-14T14:20:43.755124Z","iopub.status.idle":"2023-07-14T14:20:44.446124Z","shell.execute_reply.started":"2023-07-14T14:20:43.755096Z","shell.execute_reply":"2023-07-14T14:20:44.444928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"models","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:44.449101Z","iopub.execute_input":"2023-07-14T14:20:44.449883Z","iopub.status.idle":"2023-07-14T14:20:44.460413Z","shell.execute_reply.started":"2023-07-14T14:20:44.449827Z","shell.execute_reply":"2023-07-14T14:20:44.456524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Baseline models: CountVectorizer and TfIdfVectorizer","metadata":{}},{"cell_type":"code","source":"pd.DataFrame(models).T","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:44.462188Z","iopub.execute_input":"2023-07-14T14:20:44.462622Z","iopub.status.idle":"2023-07-14T14:20:44.486878Z","shell.execute_reply.started":"2023-07-14T14:20:44.462582Z","shell.execute_reply":"2023-07-14T14:20:44.484772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Feature engineering","metadata":{}},{"cell_type":"markdown","source":"Let's try some other techniques, let's first try feature engineering.","metadata":{}},{"cell_type":"markdown","source":"From other notebooks, we can opt to create:\n    \n**word_count**: number of words in text  \n**unique_word_count**: number of unique words in text  \n**stop_word_count**: number of stop words in text  \n**url_count**: number of urls in text  \n**mean_word_length**: average character count in words  \n**char_count**: number of characters in text  \n**punctuation_count**: number of punctuations in text  \n**hashtag_count**: number of hashtags (#) in text  \n**mention_count**: number of mentions (@) in text  ","metadata":{"execution":{"iopub.status.busy":"2023-07-13T15:09:23.853630Z","iopub.execute_input":"2023-07-13T15:09:23.854019Z","iopub.status.idle":"2023-07-13T15:09:23.863530Z","shell.execute_reply.started":"2023-07-13T15:09:23.853989Z","shell.execute_reply":"2023-07-13T15:09:23.861444Z"}}},{"cell_type":"code","source":"from wordcloud import STOPWORDS","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:44.489143Z","iopub.execute_input":"2023-07-14T14:20:44.489504Z","iopub.status.idle":"2023-07-14T14:20:44.533215Z","shell.execute_reply.started":"2023-07-14T14:20:44.489469Z","shell.execute_reply":"2023-07-14T14:20:44.532100Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p1_train","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:44.535454Z","iopub.execute_input":"2023-07-14T14:20:44.535776Z","iopub.status.idle":"2023-07-14T14:20:44.558667Z","shell.execute_reply.started":"2023-07-14T14:20:44.535745Z","shell.execute_reply":"2023-07-14T14:20:44.557383Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def feature_engineer(df):\n    df['word_count'] = df['text'].apply(lambda x: len(str(x).split()))\n    df['unique_word_count'] = df['text'].apply(lambda x: len(set(str(x).split())))\n    df['stop_word_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n    df['url_count'] = df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n    df['mean_word_length'] = df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n    df['char_count'] = df['text'].apply(lambda x: len(str(x)))\n    df['punctuation_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n    df['hashtag_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n    df['mention_count'] = df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:44.562510Z","iopub.execute_input":"2023-07-14T14:20:44.563277Z","iopub.status.idle":"2023-07-14T14:20:44.578974Z","shell.execute_reply.started":"2023-07-14T14:20:44.563241Z","shell.execute_reply":"2023-07-14T14:20:44.577740Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p2_train = feature_engineer(p1_train)\np2_test = feature_engineer(p1_test)\np1_train.head()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:44.580773Z","iopub.execute_input":"2023-07-14T14:20:44.581184Z","iopub.status.idle":"2023-07-14T14:20:45.510643Z","shell.execute_reply.started":"2023-07-14T14:20:44.581150Z","shell.execute_reply":"2023-07-14T14:20:45.509524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def clean_text(df):\n    df['cleaned_text'] = df['text'].apply(lambda x: x.lower())\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.translate(str.maketrans('','', string.punctuation)))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:45.514920Z","iopub.execute_input":"2023-07-14T14:20:45.517760Z","iopub.status.idle":"2023-07-14T14:20:45.526187Z","shell.execute_reply.started":"2023-07-14T14:20:45.517722Z","shell.execute_reply":"2023-07-14T14:20:45.525321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"engineered_features = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length', 'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\nDISASTER = p2_train['target'] == 1\n\nfig, axes = plt.subplots(len(engineered_features), 2, figsize = (20, 10))\nfor i, feat in enumerate(engineered_features):\n    sns.histplot(p2_train.loc[~DISASTER][feat], label = 'Not Disaster', ax = axes[i][0], color = 'green')\n    sns.histplot(p2_train.loc[DISASTER][feat], label = 'Disaster', ax = axes[i][0], color = 'red')\n    \n    sns.histplot(p2_train[feat], label = 'Training', ax = axes[i][1])\n    sns.histplot(p2_test[feat], label = 'Test', ax = axes[i][1])\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis = 'x', labelsize = 6)\n        axes[i][j].tick_params(axis = 'y', labelsize = 6)\n        axes[i][j].legend()\n        \n    axes[i][0].set_title(f'{feat} Target Distribution in Training Set', fontsize = 8)\n    axes[i][1].set_title(f'{feat} Training & Test Set Distribution', fontsize = 8)\n\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:45.528887Z","iopub.execute_input":"2023-07-14T14:20:45.529850Z","iopub.status.idle":"2023-07-14T14:20:55.240929Z","shell.execute_reply.started":"2023-07-14T14:20:45.529814Z","shell.execute_reply":"2023-07-14T14:20:55.239958Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport unicodedata\n\nimport nltk\n#nltk.download('wordnet', force = True)\n\nstemmer = PorterStemmer()\nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(df):\n    df['cleaned_text'] = df['text'].apply(lambda x: x.lower())\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n    \n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: [w for w in x.split() if w not in STOPWORDS])\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x))\n    \n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: [stemmer.stem(w) for w in x.split()])\n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x))\n    \n    # Does not work for some reason\n    # df['cleaned_text'] = df['cleaned_text'].apply(lambda x: [lemmatizer.lemmatize(w, pos = 'v') for w in x.split()])\n    # df['cleaned_text'] = df['cleaned_text'].apply(lambda x: ' '.join(x))\n    \n    df['cleaned_text'] = df['cleaned_text'].apply(lambda x: unicodedata.normalize('NFKD', x).encode('ascii', 'ignore').decode('utf-8'))\n    \n    return df","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:55.241978Z","iopub.execute_input":"2023-07-14T14:20:55.242360Z","iopub.status.idle":"2023-07-14T14:20:56.184027Z","shell.execute_reply.started":"2023-07-14T14:20:55.242326Z","shell.execute_reply":"2023-07-14T14:20:56.183037Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_p2_train = clean_text(p2_train)\ncleaned_p2_test = clean_text(p2_test)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:56.185407Z","iopub.execute_input":"2023-07-14T14:20:56.185892Z","iopub.status.idle":"2023-07-14T14:20:59.983916Z","shell.execute_reply.started":"2023-07-14T14:20:56.185840Z","shell.execute_reply":"2023-07-14T14:20:59.982909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cleaned_p2_train","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:20:59.985530Z","iopub.execute_input":"2023-07-14T14:20:59.986013Z","iopub.status.idle":"2023-07-14T14:21:00.009821Z","shell.execute_reply.started":"2023-07-14T14:20:59.985975Z","shell.execute_reply":"2023-07-14T14:21:00.008567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Re-run Base Models","metadata":{}},{"cell_type":"code","source":"p3_train = cleaned_p2_train['cleaned_text']\np3_test = cleaned_p2_test['cleaned_text']","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:00.011771Z","iopub.execute_input":"2023-07-14T14:21:00.012215Z","iopub.status.idle":"2023-07-14T14:21:00.021446Z","shell.execute_reply.started":"2023-07-14T14:21:00.012178Z","shell.execute_reply":"2023-07-14T14:21:00.020552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p3_train","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:00.022641Z","iopub.execute_input":"2023-07-14T14:21:00.023411Z","iopub.status.idle":"2023-07-14T14:21:00.035382Z","shell.execute_reply.started":"2023-07-14T14:21:00.023378Z","shell.execute_reply":"2023-07-14T14:21:00.034509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_2, X_val_2, y_train_2, y_val_2 = train_test_split(p3_train, y, test_size = 0.2, random_state = 42, shuffle = True)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:00.036460Z","iopub.execute_input":"2023-07-14T14:21:00.037520Z","iopub.status.idle":"2023-07-14T14:21:00.045997Z","shell.execute_reply.started":"2023-07-14T14:21:00.037484Z","shell.execute_reply":"2023-07-14T14:21:00.045096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_2","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:00.047545Z","iopub.execute_input":"2023-07-14T14:21:00.047895Z","iopub.status.idle":"2023-07-14T14:21:00.058230Z","shell.execute_reply.started":"2023-07-14T14:21:00.047844Z","shell.execute_reply":"2023-07-14T14:21:00.057302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Initialize vectorizers and pipelines\ncount_vec = CountVectorizer()\ntfidf_vec = TfidfVectorizer()\n\npipe_countvec = make_pipeline(count_vec, MultinomialNB())\npipe_tfidfvec = make_pipeline(tfidf_vec, MultinomialNB())\n\n# Fit the models\npipe_countvec.fit(X_train_2, y_train_2)\npipe_tfidfvec.fit(X_train_2, y_train_2)\n\nmodels = {}\n\n# Predict and calculate results\nmodels['countvec_multiNB'] = calculate_results(y_val_2, pipe_countvec.predict(X_val_2))\nmodels['tfidf_multiNB'] = calculate_results(y_val_2, pipe_tfidfvec.predict(X_val_2))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:00.067996Z","iopub.execute_input":"2023-07-14T14:21:00.068710Z","iopub.status.idle":"2023-07-14T14:21:00.420658Z","shell.execute_reply.started":"2023-07-14T14:21:00.068678Z","shell.execute_reply":"2023-07-14T14:21:00.419666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(models).T","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:00.422350Z","iopub.execute_input":"2023-07-14T14:21:00.422771Z","iopub.status.idle":"2023-07-14T14:21:00.435952Z","shell.execute_reply.started":"2023-07-14T14:21:00.422736Z","shell.execute_reply":"2023-07-14T14:21:00.434329Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Guess it didn't really help, noticeably there was a marginal increase on accuracy for countvec_multiNB.","metadata":{}},{"cell_type":"markdown","source":"# TensorFlow","metadata":{}},{"cell_type":"markdown","source":"We first set up our data pipeline for TensorFlow.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import TextVectorization","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:00.437390Z","iopub.execute_input":"2023-07-14T14:21:00.438367Z","iopub.status.idle":"2023-07-14T14:21:16.072230Z","shell.execute_reply.started":"2023-07-14T14:21:00.438333Z","shell.execute_reply":"2023-07-14T14:21:16.071180Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from collections import Counter\n\nwords = p1_train['text'].str.split().sum()\n\n# Count the unique words\nvocab_size = len(Counter(words))\n\nprint('Vocabulary size:', vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:16.073661Z","iopub.execute_input":"2023-07-14T14:21:16.074454Z","iopub.status.idle":"2023-07-14T14:21:18.857315Z","shell.execute_reply.started":"2023-07-14T14:21:16.074425Z","shell.execute_reply":"2023-07-14T14:21:18.856304Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(vocab_size)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:18.858654Z","iopub.execute_input":"2023-07-14T14:21:18.860136Z","iopub.status.idle":"2023-07-14T14:21:18.866232Z","shell.execute_reply.started":"2023-07-14T14:21:18.860099Z","shell.execute_reply":"2023-07-14T14:21:18.865289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"seq_len = int(np.percentile(p2_train['word_count'], 95))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:18.867750Z","iopub.execute_input":"2023-07-14T14:21:18.868475Z","iopub.status.idle":"2023-07-14T14:21:18.901725Z","shell.execute_reply.started":"2023-07-14T14:21:18.868440Z","shell.execute_reply":"2023-07-14T14:21:18.900811Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_tokens = 32000\n\ntext_vec = TextVectorization(max_tokens = max_tokens,\n                            output_sequence_length = seq_len)\ntext_vec.adapt(X_train_1)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:18.903216Z","iopub.execute_input":"2023-07-14T14:21:18.903626Z","iopub.status.idle":"2023-07-14T14:21:25.494337Z","shell.execute_reply.started":"2023-07-14T14:21:18.903591Z","shell.execute_reply":"2023-07-14T14:21:25.493268Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\ntarget_sentence = random.choice(p1_train['text'])\nprint(f\"Text:\\n{target_sentence}\")\nprint(f\"\\nLength of text: {len(target_sentence.split())}\")\nprint(f\"\\nVectorized text:\\n{text_vec([target_sentence])}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:25.495739Z","iopub.execute_input":"2023-07-14T14:21:25.498147Z","iopub.status.idle":"2023-07-14T14:21:25.621580Z","shell.execute_reply.started":"2023-07-14T14:21:25.498117Z","shell.execute_reply":"2023-07-14T14:21:25.619826Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text_vocab = text_vec.get_vocabulary()\nprint(f\"Number of words in vocabulary: {len(text_vocab)}\"), \nprint(f\"Most common words in the vocabulary: {text_vocab[:5]}\")\nprint(f\"Least common words in the vocabulary: {text_vocab[-5:]}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:25.623100Z","iopub.execute_input":"2023-07-14T14:21:25.623481Z","iopub.status.idle":"2023-07-14T14:21:25.686557Z","shell.execute_reply.started":"2023-07-14T14:21:25.623444Z","shell.execute_reply":"2023-07-14T14:21:25.685517Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"token_embed = layers.Embedding(input_dim = len(text_vocab),\n                              output_dim = 128,\n                              mask_zero = True,\n                              name = 'token_embed')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:25.688096Z","iopub.execute_input":"2023-07-14T14:21:25.688526Z","iopub.status.idle":"2023-07-14T14:21:25.696434Z","shell.execute_reply.started":"2023-07-14T14:21:25.688491Z","shell.execute_reply":"2023-07-14T14:21:25.695323Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = tf.data.Dataset.from_tensor_slices((X_train_1, y_train_1))\nval_data = tf.data.Dataset.from_tensor_slices((X_val_1, y_val_1))\n\ntrain_data = train_data.batch(32).prefetch(tf.data.AUTOTUNE)\nval_data = val_data.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:25.698075Z","iopub.execute_input":"2023-07-14T14:21:25.698421Z","iopub.status.idle":"2023-07-14T14:21:25.722279Z","shell.execute_reply.started":"2023-07-14T14:21:25.698389Z","shell.execute_reply":"2023-07-14T14:21:25.721307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Let's do a simple neural network first.","metadata":{}},{"cell_type":"markdown","source":"## Model 1: Conv1D Network","metadata":{}},{"cell_type":"code","source":"inputs = layers.Input(shape = (1,), dtype = tf.string)\ntext_vectors = text_vec(inputs)\ntoken_embeddings = token_embed(text_vectors)\nx = layers.Conv1D(64, kernel_size = 5, padding = 'same', activation = 'relu')(token_embeddings)\nx = layers.GlobalMaxPooling1D()(x)\noutputs = layers.Dense(1, activation = 'sigmoid')(x)\nmodel_1 = tf.keras.Model(inputs, outputs)\n\nmodel_1.compile(loss = 'binary_crossentropy',\n               optimizer = tf.keras.optimizers.Adam(),\n               metrics = ['accuracy'])","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:25.724129Z","iopub.execute_input":"2023-07-14T14:21:25.724562Z","iopub.status.idle":"2023-07-14T14:21:25.854454Z","shell.execute_reply.started":"2023-07-14T14:21:25.724526Z","shell.execute_reply":"2023-07-14T14:21:25.853530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.summary()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:25.857692Z","iopub.execute_input":"2023-07-14T14:21:25.857968Z","iopub.status.idle":"2023-07-14T14:21:25.882226Z","shell.execute_reply.started":"2023-07-14T14:21:25.857944Z","shell.execute_reply":"2023-07-14T14:21:25.881447Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_history = model_1.fit(train_data,\n                              epochs = 10,\n                              validation_data = val_data,\n                              verbose = 0\n                             )","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:21:25.883196Z","iopub.execute_input":"2023-07-14T14:21:25.883801Z","iopub.status.idle":"2023-07-14T14:22:17.418722Z","shell.execute_reply.started":"2023-07-14T14:21:25.883768Z","shell.execute_reply":"2023-07-14T14:22:17.417609Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1.evaluate(val_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:22:17.423173Z","iopub.execute_input":"2023-07-14T14:22:17.423460Z","iopub.status.idle":"2023-07-14T14:22:17.742854Z","shell.execute_reply.started":"2023-07-14T14:22:17.423434Z","shell.execute_reply":"2023-07-14T14:22:17.741912Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_pred_probs = model_1.predict(val_data)\nmodel_1_pred_probs","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:22:17.744283Z","iopub.execute_input":"2023-07-14T14:22:17.744648Z","iopub.status.idle":"2023-07-14T14:22:18.055853Z","shell.execute_reply.started":"2023-07-14T14:22:17.744613Z","shell.execute_reply":"2023-07-14T14:22:18.054809Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_preds = tf.cast(tf.round(model_1_pred_probs), tf.int32)\nmodel_1_preds","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:22:18.057430Z","iopub.execute_input":"2023-07-14T14:22:18.057780Z","iopub.status.idle":"2023-07-14T14:22:18.073654Z","shell.execute_reply.started":"2023-07-14T14:22:18.057747Z","shell.execute_reply":"2023-07-14T14:22:18.072762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pd.DataFrame(model_1_preds.numpy()).value_counts()","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:22:18.075254Z","iopub.execute_input":"2023-07-14T14:22:18.075689Z","iopub.status.idle":"2023-07-14T14:22:18.087166Z","shell.execute_reply.started":"2023-07-14T14:22:18.075657Z","shell.execute_reply":"2023-07-14T14:22:18.086053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_1_results = calculate_results(y_val_1,\n                                   model_1_preds.numpy())\nmodels['model_1'] = model_1_results\npd.DataFrame(models).T","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:22:18.088636Z","iopub.execute_input":"2023-07-14T14:22:18.089211Z","iopub.status.idle":"2023-07-14T14:22:18.113591Z","shell.execute_reply.started":"2023-07-14T14:22:18.089178Z","shell.execute_reply":"2023-07-14T14:22:18.112647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 2: Transfer Learning","metadata":{}},{"cell_type":"code","source":"import tensorflow_hub as hub\ntf_hub_embedding_layer = hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\",\n                                        trainable=False,\n                                        name=\"universal_sentence_encoder\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:27:00.503045Z","iopub.execute_input":"2023-07-14T15:27:00.503952Z","iopub.status.idle":"2023-07-14T15:27:20.620813Z","shell.execute_reply.started":"2023-07-14T15:27:00.503919Z","shell.execute_reply":"2023-07-14T15:27:20.619775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = layers.Input(shape=[], dtype=tf.string)\npretrained_embedding = tf_hub_embedding_layer(inputs) # tokenize text and create embedding\nx = layers.Dense(128, activation = 'relu')(pretrained_embedding)\nx = layers.Dropout(0.5)(x)\nx = layers.Dense(128, activation = 'relu')(x)\nx = layers.Dropout(0.5)(x)\noutputs = layers.Dense(1, activation=\"sigmoid\")(x) # create the output layer\nmodel_2 = tf.keras.Model(inputs=inputs,\n                        outputs=outputs)\n\n# Compile the model\nmodel_2.compile(loss=\"binary_crossentropy\",\n                optimizer=tf.keras.optimizers.Adam(),\n                metrics=[\"mse\"])\n\ncheckpoint_dir = \"kaggle/working/\"\ncheckpoint_filename = \"model.ckpt\"\n\ncheckpoint_path = os.path.join(checkpoint_dir, checkpoint_filename)\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_mse', mode='min', verbose=1, patience=10)\n\nmodel_ckpt = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_path,\n                                                monitor = 'val_mse',\n                                                save_best_only = True,\n                                                save_weights_only = True,\n                                                verbose = 0)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:27:20.623028Z","iopub.execute_input":"2023-07-14T15:27:20.623412Z","iopub.status.idle":"2023-07-14T15:27:20.695085Z","shell.execute_reply.started":"2023-07-14T15:27:20.623378Z","shell.execute_reply":"2023-07-14T15:27:20.693645Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"checkpoint_path","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:27:20.695986Z","iopub.status.idle":"2023-07-14T15:27:20.696664Z","shell.execute_reply.started":"2023-07-14T15:27:20.696420Z","shell.execute_reply":"2023-07-14T15:27:20.696445Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history_2 = model_2.fit(train_data,\n                       epochs = 100,\n                        validation_data = val_data,\n                        callbacks = [model_ckpt],\n                        verbose = 0\n                       )","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:22:37.159530Z","iopub.execute_input":"2023-07-14T14:22:37.159848Z","iopub.status.idle":"2023-07-14T14:27:46.534841Z","shell.execute_reply.started":"2023-07-14T14:22:37.159821Z","shell.execute_reply":"2023-07-14T14:27:46.533820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.load_weights(checkpoint_path)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:46.536655Z","iopub.execute_input":"2023-07-14T14:27:46.537044Z","iopub.status.idle":"2023-07-14T14:27:47.769915Z","shell.execute_reply.started":"2023-07-14T14:27:46.537007Z","shell.execute_reply":"2023-07-14T14:27:47.768798Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2.evaluate(val_data)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:47.771367Z","iopub.execute_input":"2023-07-14T14:27:47.771783Z","iopub.status.idle":"2023-07-14T14:27:48.411766Z","shell.execute_reply.started":"2023-07-14T14:27:47.771746Z","shell.execute_reply":"2023-07-14T14:27:48.410812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2_preds_prob = model_2.predict(val_data)\nmodel_2_preds_prob","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:48.413602Z","iopub.execute_input":"2023-07-14T14:27:48.414007Z","iopub.status.idle":"2023-07-14T14:27:49.369895Z","shell.execute_reply.started":"2023-07-14T14:27:48.413971Z","shell.execute_reply":"2023-07-14T14:27:49.368808Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2_preds = tf.cast(tf.round(model_2_preds_prob), tf.int32)\nmodel_2_preds","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:49.371496Z","iopub.execute_input":"2023-07-14T14:27:49.371831Z","iopub.status.idle":"2023-07-14T14:27:49.382202Z","shell.execute_reply.started":"2023-07-14T14:27:49.371803Z","shell.execute_reply":"2023-07-14T14:27:49.381097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_2_results = calculate_results(y_val_1, model_2_preds)\nmodels['model_2'] = model_2_results\npd.DataFrame(models).T","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:49.383923Z","iopub.execute_input":"2023-07-14T14:27:49.385121Z","iopub.status.idle":"2023-07-14T14:27:49.413079Z","shell.execute_reply.started":"2023-07-14T14:27:49.385084Z","shell.execute_reply":"2023-07-14T14:27:49.412036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test['text']","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:49.414670Z","iopub.execute_input":"2023-07-14T14:27:49.415326Z","iopub.status.idle":"2023-07-14T14:27:49.424691Z","shell.execute_reply.started":"2023-07-14T14:27:49.415289Z","shell.execute_reply":"2023-07-14T14:27:49.423376Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = model_2.predict(test['text'])\ny_preds = tf.cast(tf.round(y_preds), tf.int32)\ny_preds","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:49.426664Z","iopub.execute_input":"2023-07-14T14:27:49.428573Z","iopub.status.idle":"2023-07-14T14:27:51.145158Z","shell.execute_reply.started":"2023-07-14T14:27:49.428542Z","shell.execute_reply":"2023-07-14T14:27:51.144127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission['target'] = y_preds\nsubmission.to_csv('submission.csv', index = False)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:51.148798Z","iopub.execute_input":"2023-07-14T14:27:51.149132Z","iopub.status.idle":"2023-07-14T14:27:51.182243Z","shell.execute_reply.started":"2023-07-14T14:27:51.149103Z","shell.execute_reply":"2023-07-14T14:27:51.181217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import datetime\nprint(f'As of {datetime.datetime.now()}, submission score on Kaggle is: 0.81734, ranked: 328.')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:27:51.184184Z","iopub.execute_input":"2023-07-14T14:27:51.184596Z","iopub.status.idle":"2023-07-14T14:27:51.190347Z","shell.execute_reply.started":"2023-07-14T14:27:51.184561Z","shell.execute_reply":"2023-07-14T14:27:51.189210Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model 3: Ensemble","metadata":{}},{"cell_type":"markdown","source":"Instead of using UniversalEncoder, let's use BERT from Transformer, I am also interested to see how this plays out.  \nBesides that, we will implement a few sklearn models and perhaps XGBoost, we will also use VotingClassifier at the end to get an ensemble model.\nFor the sklearn models, our Text Vectorizer of choice will be CountVectorizer from the above experiment results.","metadata":{"execution":{"iopub.status.busy":"2023-07-14T11:19:03.170171Z","iopub.execute_input":"2023-07-14T11:19:03.170556Z","iopub.status.idle":"2023-07-14T11:19:03.195326Z","shell.execute_reply.started":"2023-07-14T11:19:03.170523Z","shell.execute_reply":"2023-07-14T11:19:03.194321Z"}}},{"cell_type":"code","source":"pip install -U sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:35:07.170649Z","iopub.execute_input":"2023-07-14T14:35:07.171033Z","iopub.status.idle":"2023-07-14T14:35:25.330045Z","shell.execute_reply.started":"2023-07-14T14:35:07.170994Z","shell.execute_reply":"2023-07-14T14:35:25.328771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:35:02.175651Z","iopub.execute_input":"2023-07-14T14:35:02.175943Z","iopub.status.idle":"2023-07-14T14:35:02.191216Z","shell.execute_reply.started":"2023-07-14T14:35:02.175917Z","shell.execute_reply":"2023-07-14T14:35:02.190127Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission = pd.read_csv('/kaggle/input/nlp-getting-started/sample_submission.csv')\ntrain = pd.read_csv('/kaggle/input/nlp-getting-started/train.csv')\ntest = pd.read_csv('/kaggle/input/nlp-getting-started/test.csv')","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:19:39.637796Z","iopub.execute_input":"2023-07-14T16:19:39.638205Z","iopub.status.idle":"2023-07-14T16:19:39.707700Z","shell.execute_reply.started":"2023-07-14T16:19:39.638172Z","shell.execute_reply":"2023-07-14T16:19:39.706651Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\n\nmodel = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\nembeddings = model.encode(train['text'].tolist())","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:35:25.333926Z","iopub.execute_input":"2023-07-14T14:35:25.334242Z","iopub.status.idle":"2023-07-14T14:35:55.450938Z","shell.execute_reply.started":"2023-07-14T14:35:25.334213Z","shell.execute_reply":"2023-07-14T14:35:55.449930Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf = TfidfVectorizer()\ntfidf_vec = tfidf.fit_transform(train['text'])\ntfidf_vec","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:38:42.244022Z","iopub.execute_input":"2023-07-14T14:38:42.244931Z","iopub.status.idle":"2023-07-14T14:38:42.479836Z","shell.execute_reply.started":"2023-07-14T14:38:42.244896Z","shell.execute_reply":"2023-07-14T14:38:42.478816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(tfidf_vec, train['target'], test_size=0.2, random_state=42)\n\n# Repeat the process for the BERT model\nX_train_BERT, X_val_BERT, y_train_BERT, y_val_BERT = train_test_split(embeddings, train['target'], test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:50:04.784515Z","iopub.execute_input":"2023-07-14T14:50:04.785014Z","iopub.status.idle":"2023-07-14T14:50:04.811913Z","shell.execute_reply.started":"2023-07-14T14:50:04.784967Z","shell.execute_reply":"2023-07-14T14:50:04.810939Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape, X_val.shape, X_train_BERT.shape, y_val_BERT.shape","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:50:05.110644Z","iopub.execute_input":"2023-07-14T14:50:05.111129Z","iopub.status.idle":"2023-07-14T14:50:05.124236Z","shell.execute_reply.started":"2023-07-14T14:50:05.111045Z","shell.execute_reply":"2023-07-14T14:50:05.123242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Random Forest","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\n\nrf_clf = RandomForestClassifier(random_state = 42)\nrf_clf.fit(X_train, y_train)\n\ny_pred = rf_clf.predict(X_val)\nprint(f'Accuracy score: {accuracy_score(y_val, y_pred)}')\nprint(classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:38:46.935561Z","iopub.execute_input":"2023-07-14T14:38:46.936170Z","iopub.status.idle":"2023-07-14T14:39:12.897364Z","shell.execute_reply.started":"2023-07-14T14:38:46.936134Z","shell.execute_reply":"2023-07-14T14:39:12.896145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rf_clf_BERT = RandomForestClassifier(random_state = 42)\nrf_clf_BERT.fit(X_train_BERT, y_train_BERT)\n\ny_pred = rf_clf_BERT.predict(X_val_BERT)\nprint(f'Accuracy score: {accuracy_score(y_val_BERT, y_pred)}')\nprint(classification_report(y_val_BERT, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:50:29.204471Z","iopub.execute_input":"2023-07-14T14:50:29.204828Z","iopub.status.idle":"2023-07-14T14:50:40.725277Z","shell.execute_reply.started":"2023-07-14T14:50:29.204798Z","shell.execute_reply":"2023-07-14T14:50:40.724239Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We will use the rf_clf_BERT as our RandomForestClassifier of choice.","metadata":{}},{"cell_type":"markdown","source":"## Logistic Regression","metadata":{}},{"cell_type":"code","source":"from sklearn.linear_model import LogisticRegression\n\nlr_clf = LogisticRegression(random_state = 42)\nlr_clf.fit(X_train, y_train)\n\ny_pred = lr_clf.predict(X_val)\nprint(f'Accuracy score: {accuracy_score(y_val, y_pred)}')\nprint(classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:50:48.632981Z","iopub.execute_input":"2023-07-14T14:50:48.633486Z","iopub.status.idle":"2023-07-14T14:50:49.259510Z","shell.execute_reply.started":"2023-07-14T14:50:48.633447Z","shell.execute_reply":"2023-07-14T14:50:49.258354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lr_clf_BERT = LogisticRegression(random_state = 42)\nlr_clf_BERT.fit(X_train_BERT, y_train_BERT)\n\ny_pred = lr_clf_BERT.predict(X_val_BERT)\nprint(f'Accuracy score: {accuracy_score(y_val, y_pred)}')\nprint(classification_report(y_val, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:50:49.262825Z","iopub.execute_input":"2023-07-14T14:50:49.264332Z","iopub.status.idle":"2023-07-14T14:50:49.609057Z","shell.execute_reply.started":"2023-07-14T14:50:49.264296Z","shell.execute_reply":"2023-07-14T14:50:49.606481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Similar to RandomForestClassifier, the dataset using BERT to vectorise and embed came out on top.","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import RandomizedSearchCV\n\nclf = RandomForestClassifier(bootstrap=False)\n\nrf_param_grid = {\n    'max_depth': [70, 80, 90, 100, None],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [4, 5, 6],\n    'n_estimators': [200, 300, 400, 500],\n    'bootstrap': [True, False]\n    \n}\n\ngrid_search_BERT = RandomizedSearchCV(\n    estimator = clf\n    , param_distributions = rf_param_grid\n    , n_iter = 20\n    , cv = 3\n    , verbose=1\n    , random_state=42\n    , n_jobs = -1\n)\n\ngrid_search_BERT.fit(X_train_BERT, y_train_BERT)\n\n# Create a new classifier based on the best model \nprint('Highest performing parameters: ', grid_search_BERT.best_params_)\nrf_clf_BERT = grid_search_BERT.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-07-14T14:50:52.097863Z","iopub.execute_input":"2023-07-14T14:50:52.098236Z","iopub.status.idle":"2023-07-14T15:10:23.327144Z","shell.execute_reply.started":"2023-07-14T14:50:52.098204Z","shell.execute_reply":"2023-07-14T15:10:23.325928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_BERT = rf_clf_BERT.predict(X_val_BERT)\n\nprint(\"Randomised Search RF Accuracy (BERT Encoding):\", accuracy_score(y_val_BERT, y_pred_BERT), '\\n')\nprint(classification_report(y_vala_BERT, y_pred_BERT))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:16:14.132836Z","iopub.execute_input":"2023-07-14T15:16:14.133207Z","iopub.status.idle":"2023-07-14T15:16:14.345949Z","shell.execute_reply.started":"2023-07-14T15:16:14.133177Z","shell.execute_reply":"2023-07-14T15:16:14.345077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'C': [0.01, 0.1, 1, 10, 100, 1000],\n    'penalty': ['l2', None],\n    'class_weight': ['balanced', None],\n    'max_iter': [1000, 2000]\n}\n\n# These are the params which got the best results\nparam_grid = {\n    'C': [0.1]\n    , 'class_weight': [None]\n    , 'max_iter': [1000]\n    , 'penalty': ['l2']\n}\n\n# Initialize the classifier\nlog_reg_BERT = LogisticRegression()\n\ngrid_search_BERT = GridSearchCV(\n    estimator=log_reg_BERT\n    , param_grid=param_grid\n    , cv=3\n    , verbose=2\n    , n_jobs=-1\n)\ngrid_search_BERT.fit(X_train_BERT, y_train_BERT)\n\n# Create a new classifier based on the best model \nprint('Highest performing parameters from CV Grid Search: ', grid_search_BERT.best_params_)\nlog_reg_BERT = grid_search_BERT.best_estimator_","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:15:03.641997Z","iopub.execute_input":"2023-07-14T15:15:03.642751Z","iopub.status.idle":"2023-07-14T15:15:05.817732Z","shell.execute_reply.started":"2023-07-14T15:15:03.642709Z","shell.execute_reply":"2023-07-14T15:15:05.809886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = log_reg_BERT.predict(X_val_BERT)\n\nprint(\"Randomised Search LR Accuracy:\", accuracy_score(y_val_BERT, y_pred), '\\n')\nprint(classification_report(y_val_BERT, y_pred))","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:15:38.756331Z","iopub.execute_input":"2023-07-14T15:15:38.756685Z","iopub.status.idle":"2023-07-14T15:15:38.788269Z","shell.execute_reply.started":"2023-07-14T15:15:38.756656Z","shell.execute_reply":"2023-07-14T15:15:38.787130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# define the individual models\nestimators = [\n    ('Random Forest Classifier', rf_clf_BERT), \n    ('Logistic Regression Classifier', log_reg_BERT)\n]\n\n# create the ensemble model\nhard_voting_clf = VotingClassifier(estimators=estimators, voting='hard')\nsoft_voting_clf = VotingClassifier(estimators=estimators, voting='soft')\n\n# list of classifiers for easy iteration\nclassifiers = [hard_voting_clf, soft_voting_clf]\n\n# fit each classifier and print their performance\nfor clf in classifiers:\n    clf_name = clf.__class__.__name__\n    if clf == soft_voting_clf:\n        clf_name = \"Soft \" + clf_name\n    else:\n        clf_name = \"Hard \" + clf_name\n\n    # train the voting classifier\n    clf.fit(X_train_BERT, y_train_BERT)\n\n    # make predictions\n    y_pred = clf.predict(X_val_BERT)\n\n    # calculate and print accuracy score\n    accuracy = accuracy_score(y_val_BERT, y_pred)\n    print(f\"{clf_name} Accuracy: {accuracy}\")\n\n    # print classification report\n    report = classification_report(y_val_BERT, y_pred)\n    print(f\"{clf_name} Classification Report: \\n{report}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:17:35.301802Z","iopub.execute_input":"2023-07-14T15:17:35.302195Z","iopub.status.idle":"2023-07-14T15:19:29.899292Z","shell.execute_reply.started":"2023-07-14T15:17:35.302164Z","shell.execute_reply":"2023-07-14T15:19:29.895652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Using our TF model (model 2) from previous runs.","metadata":{}},{"cell_type":"code","source":"# from https://www.kaggle.com/code/fraserwtt/nlp-disaster-tweet-classification#Round-2:-Tensorflow-Modelling\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, MaxPooling1D, Conv1D\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.metrics import AUC, Precision, Recall\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import backend as K\nimport tensorflow_addons as tfa\n\ndef f1_score(y_true, y_pred): #taken from old keras source code\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives / (predicted_positives + K.epsilon())\n    recall = true_positives / (possible_positives + K.epsilon())\n    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n    return f1_val\n\n# Get the number of features (columns)\nnum_features = X_train_BERT.shape[1]\n\ndef build_nn():\n    model = Sequential()\n#     model.add(Dense(128, input_dim=num_features, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(tf.keras.layers.Reshape((num_features, 1), input_shape=(num_features,)))\n    model.add(Conv1D(activation='relu',\n        filters=64, \n        kernel_size=4, \n        strides=1,\n        padding='same'))\n    model.add(MaxPooling1D(2))\n    model.add(tf.keras.layers.Flatten())\n    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(Dropout(0.5))\n    model.add(Dense(128, activation='relu', kernel_regularizer=l2(0.001)))\n    model.add(Dense(1, activation='sigmoid'))\n\n    # Define the learning rate decay schedule\n    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n        initial_learning_rate=1e-1,\n        decay_steps=10000,\n        decay_rate=0.95)\n\n    optimizer = Adam(learning_rate=lr_schedule)\n\n    model.compile(\n        loss=tf.keras.losses.BinaryCrossentropy(from_logits=False),\n        optimizer='adam',\n        metrics=['accuracy', tfa.metrics.F1Score(num_classes=2, average='micro')]\n    )\n    return model\n\n# Define the EarlyStopping callback\nearly_stopping = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\n\n# Define the ModelCheckpoint callback\nmodel_checkpoint = ModelCheckpoint('model.h5', monitor='val_loss', mode='min', save_best_only=True, verbose=1)\n\n# Define the ReduceLROnPlateau callback\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_lr=0.00001)\n\n# Fit the model\nkeras_nn = build_nn()\nhistory = keras_nn.fit(\n    X_train_BERT, y_train_BERT, epochs=50, batch_size=8, validation_split=0.2, callbacks=[early_stopping, model_checkpoint, reduce_lr]\n)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:10:22.303602Z","iopub.execute_input":"2023-07-14T16:10:22.304029Z","iopub.status.idle":"2023-07-14T16:11:04.449589Z","shell.execute_reply.started":"2023-07-14T16:10:22.303998Z","shell.execute_reply":"2023-07-14T16:11:04.448457Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Preparing the data to tensor_slices.","metadata":{}},{"cell_type":"code","source":"!pip install scikeras[tensorflow]","metadata":{"execution":{"iopub.status.busy":"2023-07-14T15:44:13.897996Z","iopub.execute_input":"2023-07-14T15:44:13.898404Z","iopub.status.idle":"2023-07-14T15:44:26.169903Z","shell.execute_reply.started":"2023-07-14T15:44:13.898373Z","shell.execute_reply":"2023-07-14T15:44:26.168762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from scikeras.wrappers import KerasClassifier\n\nclass CustomKerasClassifier(KerasClassifier):\n    def predict(self, x, **kwargs):\n        \"\"\"Returns the class prediction of the samples\"\"\"\n        x = x.toarray() if hasattr(x, 'toarray') else np.array(x)\n        return super().predict(x, **kwargs)\n    \n    def predict_proba(self, x, **kwargs):\n        \"\"\"Returns class probabilities of the samples\"\"\"\n        x = x.toarray() if hasattr(x, 'toarray') else np.array(x)\n        proba = super().predict_proba(x, **kwargs)\n        \n        # Check if it's binary classification\n        if proba.shape[1] == 1:\n            # Assuming the single output is the probability of the positive class\n            return np.hstack([1 - proba, proba])  # shape should be (n_samples, 2)\n\n        return proba\n\nkeras_clf = CustomKerasClassifier(\n    build_nn,\n    epochs=10,\n    batch_size=8,\n    validation_split=0.2,\n    callbacks=[early_stopping, model_checkpoint, reduce_lr]\n)\nkeras_clf._estimator_type = \"classifier\"","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:12:03.558775Z","iopub.execute_input":"2023-07-14T16:12:03.559184Z","iopub.status.idle":"2023-07-14T16:12:03.569345Z","shell.execute_reply.started":"2023-07-14T16:12:03.559150Z","shell.execute_reply":"2023-07-14T16:12:03.568322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds = tf.data.Dataset.from_tensor_slices((X_train_BERT, y_train_BERT))\nval_ds = tf.data.Dataset.from_tensor_slices((X_val_BERT, y_val_BERT))\n\ntrain_ds = train_ds.batch(32).prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.batch(32).prefetch(tf.data.AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:12:08.940558Z","iopub.execute_input":"2023-07-14T16:12:08.941108Z","iopub.status.idle":"2023-07-14T16:12:08.964666Z","shell.execute_reply.started":"2023-07-14T16:12:08.941050Z","shell.execute_reply":"2023-07-14T16:12:08.963534Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define the individual models\nestimators = [\n    ('Keras Neural Network Classifier', keras_clf),\n    ('Random Forest Classifier', rf_clf_BERT), \n    ('Logistic Regression Classifier', log_reg_BERT)\n]\n\n# create the ensemble model\nhard_voting_clf = VotingClassifier(\n    estimators=estimators, voting='hard', flatten_transform=True\n)\n\nhard_voting_clf.fit(X_train_BERT, y_train_BERT)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:16:11.124169Z","iopub.execute_input":"2023-07-14T16:16:11.125102Z","iopub.status.idle":"2023-07-14T16:18:02.897885Z","shell.execute_reply.started":"2023-07-14T16:16:11.125052Z","shell.execute_reply":"2023-07-14T16:18:02.896143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_preds = hard_voting_clf.predict(X_val_BERT)\n\naccuracy = accuracy_score(y_val_BERT, y_preds)\nprint(f\"Voting Classifier Accuracy: {accuracy}\")\n\n# # print classification report\nreport = classification_report(y_val_BERT, y_preds)\nprint(f\"Classification Report:\\n{report}\")","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:19:14.804961Z","iopub.execute_input":"2023-07-14T16:19:14.805658Z","iopub.status.idle":"2023-07-14T16:19:15.749933Z","shell.execute_reply.started":"2023-07-14T16:19:14.805624Z","shell.execute_reply":"2023-07-14T16:19:15.748894Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_test_data_vectorized = model.encode(test['text'].tolist())\nsubmission = pd.DataFrame({'id': test['id'], 'target': hard_voting_clf.predict(new_test_data_vectorized)})\nsubmission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:26:20.411653Z","iopub.execute_input":"2023-07-14T16:26:20.412047Z","iopub.status.idle":"2023-07-14T16:26:24.037285Z","shell.execute_reply.started":"2023-07-14T16:26:20.412014Z","shell.execute_reply":"2023-07-14T16:26:24.035971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"output","metadata":{"execution":{"iopub.status.busy":"2023-07-14T16:25:43.345636Z","iopub.execute_input":"2023-07-14T16:25:43.346027Z","iopub.status.idle":"2023-07-14T16:25:43.363626Z","shell.execute_reply.started":"2023-07-14T16:25:43.345994Z","shell.execute_reply":"2023-07-14T16:25:43.362005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}